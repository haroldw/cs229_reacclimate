{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pytorch model\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LTSM\n",
    "import util\n",
    "import time\n",
    "import pdb\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "BATCH_SIZE = 64 * 4\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 30\n",
    "best_valid_loss = float('inf')\n",
    "tPath = '../twitter/data/'\n",
    "trainFile = './test.csv'\n",
    "testFile = './test.csv'\n",
    "valFile = './val.csv'\n",
    "\n",
    "df = pd.read_csv(valFile)\n",
    "usrGrpCnt = len(df.columns) - 1\n",
    "sentCategoryCnt = len(df[df.columns[-1]].unique())\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True, lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "csvFields = [   ('text', TEXT) ]\n",
    "for userGrp in range( usrGrpCnt ):\n",
    "    label = 'group%s' % userGrp\n",
    "    csvFields.append( ( label, LABEL ) )\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                path='.', \n",
    "                train=trainFile,\n",
    "                validation=valFile, \n",
    "                test=testFile, \n",
    "                format='csv',\n",
    "                fields=csvFields,\n",
    "                skip_header=True,\n",
    "            )\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "INPUT_DIM = 25002\n",
    "PAD_IDX = 1\n",
    "modelGrp0 = LTSM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, 1*sentCategoryCnt, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "modelGrp1 = LTSM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, 1*sentCategoryCnt, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "model_group_zero = modelGrp0.to(device)\n",
    "model_group_one = modelGrp1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_group_zero.load_state_dict(torch.load('lstm_model_group0.pt'))\n",
    "model_group_one.load_state_dict(torch.load('lstm_model_group1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example engagement scores:\n",
      "Climate change is terrible:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"example engagement scores:\")\n",
    "first_ex_engagement = util.predict_engagement(model_group_zero, 'Climate change is terrible', TEXT, device).item()\n",
    "second_ex_engagement = util.predict_engagement(model_group_one, 'We need to act now to fix climate change', TEXT, device).item()\n",
    "print('\"Climate change is terrible\": ', first_ex_engagement)\n",
    "print('\"We need to act now to fix climate change\": ', second_ex_engagement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words and replacements are\n",
      "delta: 0.0, originial: one, new: matchless\n",
      "delta: 0.0, originial: biggest, new: large\n",
      "delta: 0.0, originial: biggest, new: big\n",
      "delta: 0.0, originial: biggest, new: bad\n",
      "delta: 0.0, originial: biggest, new: boastful\n",
      "delta: 0.0, originial: biggest, new: adult\n",
      "delta: 0.0, originial: dangers, new: danger\n",
      "delta: 0.0, originial: dangers, new: risk\n",
      "delta: 0.0, originial: may, new: May\n",
      "delta: 0.0, originial: may, new: whitethorn\n",
      "delta: 0.0, originial: big, new: large\n",
      "delta: 0.0, originial: big, new: bad\n",
      "delta: 0.0, originial: big, new: boastful\n",
      "delta: 0.0, originial: big, new: adult\n",
      "delta: 0.0, originial: big, new: boastfully\n",
      "delta: 0.0, originial: questions, new: question\n",
      "delta: 0.0, originial: questions, new: doubt\n",
      "delta: 0.0, originial: questions, new: motion\n",
      "delta: 0.0, originial: questions, new: interrogate\n",
      "delta: 0.0, originial: questions, new: interview\n"
     ]
    }
   ],
   "source": [
    "# iterate through words in a unique corpus dictionary\n",
    "unique_words = set()\n",
    "word_to_tweets = defaultdict(list)\n",
    "tweet_to_engagement = defaultdict(list)\n",
    "alt_tweet_to_engagement = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "tweet_file = pd.read_csv(testFile)\n",
    "tweets = tweet_file['text']\n",
    "tweets = tweets[:3]\n",
    "\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    # TODO: replace with model here\n",
    "    group_one_engagement = util.predict_engagement(model_group_one, tweet, TEXT, device).item()\n",
    "    group_zero_engagement = util.predict_engagement(model_group_zero, tweet, TEXT, device).item()\n",
    "    for word in filtered_words:\n",
    "        tweet_to_engagement[word].append((group_zero_engagement, group_one_engagement))\n",
    "\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    unique_words = unique_words.union(filtered_words)\n",
    "    for word in filtered_words:\n",
    "        word_to_tweets[word].append(tweet_idx)\n",
    "        \n",
    "\n",
    "# for each word, get 5 alternatives\n",
    "for word in unique_words:\n",
    "    syns = wordnet.synsets(word) \n",
    "    alternatives = []\n",
    "    for synonym in syns:\n",
    "        syn = synonym.lemmas()[0].name()\n",
    "        if syn != word and syn not in alternatives:\n",
    "            alternatives.append(syn)\n",
    "        if len(alternatives) == 5:\n",
    "            break\n",
    "    tweets_with_word = word_to_tweets[word]\n",
    "    # for each alt, iterate through tweets that contain this word, substitute word with alt\n",
    "    for alt in alternatives:\n",
    "        for tweet_idx in tweets_with_word:\n",
    "            tweet = tweets[tweet_idx]\n",
    "            alt_tweet = tweet.replace(word, alt)\n",
    "            # recompute engagement score delta across all user groups\n",
    "            # TODO: replace with model here\n",
    "            group_one_engagement = util.predict_engagement(model_group_one, alt_tweet, TEXT, device).item()\n",
    "            group_zero_engagement = util.predict_engagement(model_group_zero, alt_tweet, TEXT, device).item()\n",
    "            alt_tweet_to_engagement[word][alt].append((group_one_engagement, group_zero_engagement))\n",
    "# record alt with highest delta\n",
    "replacements = []\n",
    "Replacement = namedtuple('Replacement', ['delta', 'original', 'alt'])\n",
    "for word in tweet_to_engagement:\n",
    "    engagement_list = tweet_to_engagement[word]\n",
    "    avg_engagement_orig = np.mean(engagement_list, axis=0)\n",
    "    alt_words = alt_tweet_to_engagement[word]\n",
    "    for alt_word in alt_words:\n",
    "        avg_engagement_alt = np.mean(alt_words[alt_word], axis=0)\n",
    "        delta = sum(avg_engagement_alt - avg_engagement_orig)\n",
    "        replacements.append(Replacement(delta, word, alt_word))\n",
    "replacements.sort(key=lambda x: x.delta)\n",
    "# record top 10 words with highest delta and that is our answer\n",
    "print(\"top 20 words and replacements are\")\n",
    "for replacement in replacements[-20:]:\n",
    "    print(\"delta: {}, originial: {}, new: {}\".format(replacement.delta, replacement.original, replacement.alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
