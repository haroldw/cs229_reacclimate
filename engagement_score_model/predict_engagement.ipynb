{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pytorch model\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LTSM\n",
    "import util\n",
    "import time\n",
    "import pdb\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "MAX_VOCAB_SIZE = 10_000\n",
    "BATCH_SIZE = 64 * 64\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = int(256/8)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "tPath = '../twitter/data/'\n",
    "trainFile = './train.csv'\n",
    "testFile = './test.csv'\n",
    "valFile = './val.csv'\n",
    "\n",
    "df = pd.read_csv(valFile)\n",
    "usrGrpCnt = len(df.columns) - 1\n",
    "sentCategoryCnt = len(df[df.columns[-1]].unique())\n",
    "output_dim = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True, lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "csvFields = [   ('text', TEXT) ]\n",
    "for userGrp in range( usrGrpCnt ):\n",
    "    label = 'group%s' % userGrp\n",
    "    csvFields.append( ( label, LABEL ) )\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                path='.', \n",
    "                train=trainFile,\n",
    "                validation=valFile, \n",
    "                test=testFile, \n",
    "                format='csv',\n",
    "                fields=csvFields,\n",
    "                skip_header=True,\n",
    "            )\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.twitter.27B.50d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "INPUT_DIM = 10002\n",
    "PAD_IDX = 1\n",
    "modelGrp0 = LTSM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, output_dim, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "modelGrp1 = LTSM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, output_dim, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "model_group_zero = modelGrp0.to(device)\n",
    "model_group_one = modelGrp1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_group_zero.load_state_dict(torch.load('lstm_model_group0.pt'))\n",
    "model_group_one.load_state_dict(torch.load('lstm_model_group1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example engagement scores:\n",
      "\"Climate change is terrible\":  841.337646484375\n",
      "\"We need to act now to fix climate change\":  106.82893371582031\n"
     ]
    }
   ],
   "source": [
    "print(\"example engagement scores:\")\n",
    "follower_count = torch.tensor( [[0.2]] ).to(device)\n",
    "first_ex_engagement = util.predict_engagement(model_group_zero, 'Climate change is terrible', TEXT, device, follower_count).item()\n",
    "second_ex_engagement = util.predict_engagement(model_group_one, 'We need to act now to fix climate change', TEXT, device, follower_count).item()\n",
    "print('\"Climate change is terrible\": ', first_ex_engagement)\n",
    "print('\"We need to act now to fix climate change\": ', second_ex_engagement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting engagements\n",
      "engagements predictsd\n",
      "getting words\n",
      "got words\n",
      "getting alts\n",
      "len of alts  2697\n",
      "got alts\n",
      "getting replacements\n",
      "got replacements\n",
      "top 20 words and replacements are\n",
      "delta: 1792.0, originial: threatens, new: endanger\n",
      "delta: 853.333333492279, originial: hand, new: pass\n",
      "delta: 768.0, originial: several, new: respective\n",
      "delta: 768.0, originial: residence, new: residency\n",
      "delta: 768.0, originial: dump, new: shit\n",
      "delta: 768.0, originial: saying, new: suppose\n",
      "delta: 743.59375, originial: shoot, new: photograph\n",
      "delta: 743.59375, originial: shoot, new: dart\n",
      "delta: 743.0625, originial: shoot, new: film\n",
      "delta: 725.34375, originial: shoot, new: blast\n",
      "delta: 716.8000001907349, originial: saying, new: state\n",
      "delta: 683.3333332538605, originial: wreck, new: bust_up\n",
      "delta: 683.3333332538605, originial: wreck, new: shipwreck\n",
      "delta: 597.333333492279, originial: regulated, new: baffle\n",
      "delta: 576.0, originial: product, new: intersection\n",
      "delta: 576.0, originial: product, new: merchandise\n",
      "delta: 558.625, originial: situation, new: position\n",
      "delta: 536.0, originial: civilization, new: culture\n",
      "delta: 512.0, originial: strongest, new: strong\n",
      "delta: 512.0, originial: sweden, new: Sweden\n"
     ]
    }
   ],
   "source": [
    "# iterate through words in a unique corpus dictionary\n",
    "unique_words = set()\n",
    "word_to_tweets = defaultdict(list)\n",
    "tweet_to_engagement = defaultdict(list)\n",
    "alt_tweet_to_engagement = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "tweet_file = pd.read_csv(testFile)\n",
    "tweets = tweet_file['clean_text']\n",
    "followers = tweet_file['follower_count']\n",
    "tweets = tweets[:1000]\n",
    "followers = followers[:1000]\n",
    "\n",
    "inverse_box = lambda x: (x*(-0.6)+1)**(1/-0.6)\n",
    "\n",
    "print(\"predicting engagements\")\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    num_followers = torch.tensor(followers[tweet_idx]).to(device)\n",
    "    group_one_engagement = util.predict_engagement(model_group_one, tweet, TEXT, device, num_followers).item()\n",
    "    group_zero_engagement = util.predict_engagement(model_group_zero, tweet, TEXT, device, num_followers).item()\n",
    "    for word in filtered_words:\n",
    "        tweet_to_engagement[word].append(((group_zero_engagement), (group_one_engagement)))\n",
    "print(\"engagements predictsd\")\n",
    "        \n",
    "print(\"getting words\")\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    unique_words = unique_words.union(filtered_words)\n",
    "    for word in filtered_words:\n",
    "        word_to_tweets[word].append(tweet_idx)\n",
    "print(\"got words\")\n",
    "\n",
    "\n",
    "print(\"getting alts\")\n",
    "# for each word, get 5 alternatives\n",
    "print(\"len of alts \", len(unique_words))\n",
    "for word in unique_words:\n",
    "    syns = wordnet.synsets(word) \n",
    "    alternatives = []\n",
    "    for synonym in syns:\n",
    "        syn = synonym.lemmas()[0].name()\n",
    "        if syn != word and syn not in alternatives:\n",
    "            alternatives.append(syn)\n",
    "        if len(alternatives) == 5:\n",
    "            break\n",
    "    tweets_with_word = word_to_tweets[word]\n",
    "    # for each alt, iterate through tweets that contain this word, substitute word with alt\n",
    "    for alt in alternatives:\n",
    "        for tweet_idx in tweets_with_word:\n",
    "            tweet = tweets[tweet_idx]\n",
    "            num_followers = torch.tensor(followers[tweet_idx]).to(device)\n",
    "            alt_tweet = tweet.replace(word, alt)\n",
    "            # recompute engagement score delta across all user groups\n",
    "            group_one_engagement = util.predict_engagement(model_group_one, alt_tweet, TEXT, device, num_followers).item()\n",
    "            group_zero_engagement = util.predict_engagement(model_group_zero, alt_tweet, TEXT, device, num_followers).item()\n",
    "            alt_tweet_to_engagement[word][alt].append(((group_one_engagement), (group_zero_engagement)))\n",
    "print(\"got alts\")\n",
    "# record alt with highest delta\n",
    "print(\"getting replacements\")\n",
    "replacements = []\n",
    "Replacement = namedtuple('Replacement', ['delta', 'original', 'alt'])\n",
    "for word in tweet_to_engagement:\n",
    "    engagement_list = tweet_to_engagement[word]\n",
    "    avg_engagement_orig = np.mean(engagement_list, axis=0)\n",
    "    alt_words = alt_tweet_to_engagement[word]\n",
    "    for alt_word in alt_words:\n",
    "        avg_engagement_alt = np.mean(alt_words[alt_word], axis=0)\n",
    "        delta = sum(avg_engagement_alt - avg_engagement_orig)\n",
    "        replacements.append(Replacement(delta, word, alt_word))\n",
    "replacements.sort(key=lambda x: x.delta)\n",
    "print(\"got replacements\")\n",
    "# record top 10 words with highest delta and that is our answer\n",
    "print(\"top 20 words and replacements are\")\n",
    "for replacement in replacements[-20:][::-1]:\n",
    "    print(\"delta: {}, originial: {}, new: {}\".format(replacement.delta, replacement.original, replacement.alt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_tweet_to_engagement['dangers']['risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
