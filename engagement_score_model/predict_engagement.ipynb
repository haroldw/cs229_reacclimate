{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pytorch model\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models import LTSM\n",
    "import util\n",
    "import time\n",
    "import pdb\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "#Hyperparameters and config variables\n",
    "######################################################\n",
    "SEED = 1234\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "BATCH_SIZE = 64 * 4\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "N_EPOCHS = 30\n",
    "best_valid_loss = float('inf')\n",
    "tPath = '../twitter/data/'\n",
    "trainFile = './train.csv'\n",
    "testFile = './test.csv'\n",
    "valFile = './val.csv'\n",
    "\n",
    "df = pd.read_csv(valFile)\n",
    "usrGrpCnt = len(df.columns) - 1\n",
    "sentCategoryCnt = len(df[df.columns[-1]].unique())\n",
    "labelName = 'group1'\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', include_lengths = True, lower=True)\n",
    "LABEL = data.LabelField(dtype = torch.long)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "csvFields = [   ('text', TEXT) ]\n",
    "labelFields = ['group0']\n",
    "for userGrp in range( usrGrpCnt ):\n",
    "    label = 'group%s' % userGrp\n",
    "    csvFields.append( ( label, LABEL ) )\n",
    "#    labelFields.append( label )\n",
    "\n",
    "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
    "                path='.', \n",
    "                train=trainFile,\n",
    "                validation=valFile, \n",
    "                test=testFile, \n",
    "                format='csv',\n",
    "                fields=csvFields,\n",
    "                skip_header=True,\n",
    "            )\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = LTSM(MAX_VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, 1*sentCategoryCnt, \n",
    "            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_group_one = model.load_state_dict(torch.load('lstm_model_group0.pt'))\n",
    "model_group_zero = model.load_state_dict(torch.load('lstm_model_group1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IncompatibleKeys' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-42a471907cc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_engagement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_group_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Climate change is terrible'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_engagement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_group_zero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'We need to act now to fix climate change'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs229_reacclimate/engagement_score_model/util.py\u001b[0m in \u001b[0;36mpredict_engagement\u001b[0;34m(model, sentence, TEXT, device)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_engagement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mindexed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IncompatibleKeys' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "print(util.predict_engagement(model_group_one, 'Climate change is terrible', TEXT, device))\n",
    "print(util.predict_engagement(model_group_zero, 'We need to act now to fix climate change', TEXT, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Replacement(delta=0.0, original='big', alt='large'), Replacement(delta=0.0, original='big', alt='bad'), Replacement(delta=0.0, original='big', alt='boastful'), Replacement(delta=0.0, original='big', alt='adult'), Replacement(delta=0.0, original='big', alt='boastfully'), Replacement(delta=0.0, original='questions', alt='question'), Replacement(delta=0.0, original='questions', alt='doubt'), Replacement(delta=0.0, original='questions', alt='motion'), Replacement(delta=0.0, original='questions', alt='interrogate'), Replacement(delta=0.0, original='questions', alt='interview')]\n"
     ]
    }
   ],
   "source": [
    "# iterate through words in a unique corpus dictionary\n",
    "unique_words = set()\n",
    "word_to_tweets = defaultdict(list)\n",
    "tweet_to_engagement = defaultdict(list)\n",
    "alt_tweet_to_engagement = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "tweet_file = pd.read_csv(testFile)\n",
    "tweets = tweet_file['text']\n",
    "tweets = tweets[:3]\n",
    "\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    group_one_engagement = 0\n",
    "    group_zero_engagement = 1\n",
    "    # TODO: replace with model here\n",
    "#     group_one_engagement = util.predict_engagement(model_group_one, tweet, TEXT, device)\n",
    "#     group_zero_engagement = util.predict_engagement(model_group_zero, tweet, TEXT, device)\n",
    "    for word in filtered_words:\n",
    "        tweet_to_engagement[word].append((group_zero_engagement, group_one_engagement))\n",
    "\n",
    "for tweet_idx, tweet in enumerate(tweets):\n",
    "    filtered_words = [word for word in tweet.split(' ') if word not in stopwords.words('english')]\n",
    "    unique_words = unique_words.union(filtered_words)\n",
    "    for word in filtered_words:\n",
    "        word_to_tweets[word].append(tweet_idx)\n",
    "        \n",
    "\n",
    "# for each word, get 5 alternatives\n",
    "for word in unique_words:\n",
    "    syns = wordnet.synsets(word) \n",
    "    alternatives = []\n",
    "    for synonym in syns:\n",
    "        syn = synonym.lemmas()[0].name()\n",
    "        if syn != word and syn not in alternatives:\n",
    "            alternatives.append(syn)\n",
    "        if len(alternatives) == 5:\n",
    "            break\n",
    "    tweets_with_word = word_to_tweets[word]\n",
    "    # for each alt, iterate through tweets that contain this word, substitute word with alt\n",
    "    for alt in alternatives:\n",
    "        for tweet_idx in tweets_with_word:\n",
    "            tweet = tweets[tweet_idx]\n",
    "            alt_tweet = tweet.replace(word, alt)\n",
    "            # recompute engagement score delta across all user groups\n",
    "            group_one_engagement = 0\n",
    "            group_zero_engagement = 1\n",
    "            # TODO: replace with model here\n",
    "            #group_one_engagement = util.predict_engagement(model_group_one, alt_tweet, TEXT, device)\n",
    "            #group_zero_engagement = util.predict_engagement(model_group_zero, alt_tweet, TEXT, device)\n",
    "            alt_tweet_to_engagement[word][alt].append((group_one_engagement, group_zero_engagement))\n",
    "# record alt with highest delta\n",
    "replacements = []\n",
    "Replacement = namedtuple('Replacement', ['delta', 'original', 'alt'])\n",
    "for word in tweet_to_engagement:\n",
    "    engagement_list = tweet_to_engagement[word]\n",
    "    avg_engagement_orig = np.mean(engagement_list, axis=0)\n",
    "    alt_words = alt_tweet_to_engagement[word]\n",
    "    for alt_word in alt_words:\n",
    "        avg_engagement_alt = np.mean(alt_words[alt_word], axis=0)\n",
    "        delta = sum(avg_engagement_alt - avg_engagement_orig)\n",
    "        replacements.append(Replacement(delta, word, alt_word))\n",
    "replacements.sort(key=lambda x: x.delta)\n",
    "# record top 10 words with highest delta and that is our answer\n",
    "print(replacements[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
