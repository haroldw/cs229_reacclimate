{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from numpy.random import RandomState\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from datetime import timedelta  \n",
    "from sklearn.utils import shuffle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     1,
     4
    ]
   },
   "outputs": [],
   "source": [
    "# Functions for importing & cleaning relevant tweets\n",
    "def lower(s):\n",
    "    return s.lower()\n",
    "\n",
    "def tweet_imports(filename):\n",
    "    imp = pd.read_pickle(filename)\n",
    "    imp = imp.drop_duplicates()\n",
    "    imp['tweet_clean'] = imp['tweet'].str.replace('http\\S+|www.\\S+|pic.twitter.com\\S+', '', case=False)\n",
    "    imp['tweet_clean'] =imp['tweet_clean'].replace('[^A-Za-z0-9 ]+','',regex=True)\n",
    "    imp['tweet_clean'] = imp['tweet_clean'].apply(lower)#map(lambda x: x.lower(), imp['tweet_clean'])\n",
    "    imp['date'] = pd.to_datetime(imp['date'])\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54453, 12)\n",
      "(4233, 12)\n",
      "(54453, 12)\n",
      "(4233, 12)\n"
     ]
    }
   ],
   "source": [
    "# Collect all tweets from every user into 2 groups of affirming and denying tweets, add label (1 vs. -1)\n",
    "all_affirm_tweets = []\n",
    "all_deny_tweets = []\n",
    "\n",
    "for filename in os.listdir(os.getcwd()+'/affirm_tweets/'):\n",
    "    tweets_per_user = pd.read_pickle(os.getcwd()+'/affirm_tweets/'+filename)\n",
    "    all_affirm_tweets.append(tweets_per_user)\n",
    "for filename in os.listdir(os.getcwd()+'/deny_tweets/'):\n",
    "    tweets_per_user = pd.read_pickle(os.getcwd()+'/deny_tweets/'+filename)\n",
    "    all_deny_tweets.append(tweets_per_user)\n",
    "    \n",
    "affirm_tweets = pd.concat(all_affirm_tweets)\n",
    "affirm_tweets['label'] = [1]*affirm_tweets.shape[0]\n",
    "deny_tweets = pd.concat(all_deny_tweets)\n",
    "deny_tweets['label'] = [-1]*deny_tweets.shape[0]\n",
    "print(affirm_tweets.shape)\n",
    "print(deny_tweets.shape)\n",
    "\n",
    "affirm_tweets.drop_duplicates(subset =\"id\", keep = 'first', inplace = True)\n",
    "deny_tweets.drop_duplicates(subset =\"id\", keep = 'first', inplace = True)\n",
    "print(affirm_tweets.shape)\n",
    "print(deny_tweets.shape)\n",
    "\n",
    "affirm_tweets.to_pickle('all_affirm_tweets.pkl')\n",
    "deny_tweets.to_pickle('all_deny_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_affirm_tweets = tweet_imports('all_affirm_tweets.pkl')\n",
    "cleaned_deny_tweets = tweet_imports('all_deny_tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4233, 13)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_deny_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>tweet</th>\n",
       "      <th>mentions</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>username</th>\n",
       "      <th>search_term</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1184173107118071808</td>\n",
       "      <td>2019-10-15</td>\n",
       "      <td>11:24:04</td>\n",
       "      <td>New video from @ClimateAdam! \\nTipping Points:...</td>\n",
       "      <td>['climateadam']</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>global warming</td>\n",
       "      <td>-1</td>\n",
       "      <td>new video from climateadam tipping points coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1177890151739076611</td>\n",
       "      <td>2019-09-28</td>\n",
       "      <td>03:17:50</td>\n",
       "      <td>How 'organized climate change denial' shapes p...</td>\n",
       "      <td>['johnfocook']</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>global warming</td>\n",
       "      <td>-1</td>\n",
       "      <td>how organized climate change denial shapes pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1172767532022996992</td>\n",
       "      <td>2019-09-14</td>\n",
       "      <td>00:02:23</td>\n",
       "      <td>Opinion: Can we please base our climate change...</td>\n",
       "      <td>['naomioreskes', 'latimes']</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>global warming</td>\n",
       "      <td>-1</td>\n",
       "      <td>opinion can we please base our climate change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1165319215760326656</td>\n",
       "      <td>2019-08-24</td>\n",
       "      <td>10:45:26</td>\n",
       "      <td>Earth Stopped Getting Greener 20 Years Ago - D...</td>\n",
       "      <td>['sciam']</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>global warming</td>\n",
       "      <td>-1</td>\n",
       "      <td>earth stopped getting greener 20 years ago  de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1160554501360824320</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>07:12:09</td>\n",
       "      <td>Why Solar Activity And Cosmic Rays Can’t Expla...</td>\n",
       "      <td>['drshepherd2013']</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>global warming</td>\n",
       "      <td>-1</td>\n",
       "      <td>why solar activity and cosmic rays cant explai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1092105203137040385</td>\n",
       "      <td>2019-02-03</td>\n",
       "      <td>08:59:05</td>\n",
       "      <td>\"I have never been an environmentalist. I don’...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>environmentalist</td>\n",
       "      <td>-1</td>\n",
       "      <td>i have never been an environmentalist i dont e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>991935404780785664</td>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>00:00:04</td>\n",
       "      <td>The phrase “Mormon environmentalist” is not on...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>environmentalist</td>\n",
       "      <td>-1</td>\n",
       "      <td>the phrase mormon environmentalist is not one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>944387122563354625</td>\n",
       "      <td>2017-12-22</td>\n",
       "      <td>18:00:10</td>\n",
       "      <td>Iceland's new prime minister is a feminist and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>47</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>environmentalist</td>\n",
       "      <td>-1</td>\n",
       "      <td>icelands new prime minister is a feminist and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>898393806659358720</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>20:59:09</td>\n",
       "      <td>Climate change is the biggest story on any edi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>environmentalist</td>\n",
       "      <td>-1</td>\n",
       "      <td>climate change is the biggest story on any edi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>106601860076544000</td>\n",
       "      <td>2011-08-24</td>\n",
       "      <td>22:40:38</td>\n",
       "      <td>@EileenOttawa thanx Eileen. Don't meet many fe...</td>\n",
       "      <td>['eileenottawa']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>skepticscience</td>\n",
       "      <td>environmentalist</td>\n",
       "      <td>-1</td>\n",
       "      <td>eileenottawa thanx eileen dont meet many fello...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4233 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       date      time  \\\n",
       "0   1184173107118071808 2019-10-15  11:24:04   \n",
       "1   1177890151739076611 2019-09-28  03:17:50   \n",
       "2   1172767532022996992 2019-09-14  00:02:23   \n",
       "3   1165319215760326656 2019-08-24  10:45:26   \n",
       "4   1160554501360824320 2019-08-11  07:12:09   \n",
       "..                  ...        ...       ...   \n",
       "1   1092105203137040385 2019-02-03  08:59:05   \n",
       "2    991935404780785664 2018-05-03  00:00:04   \n",
       "3    944387122563354625 2017-12-22  18:00:10   \n",
       "5    898393806659358720 2017-08-17  20:59:09   \n",
       "6    106601860076544000 2011-08-24  22:40:38   \n",
       "\n",
       "                                                tweet  \\\n",
       "0   New video from @ClimateAdam! \\nTipping Points:...   \n",
       "1   How 'organized climate change denial' shapes p...   \n",
       "2   Opinion: Can we please base our climate change...   \n",
       "3   Earth Stopped Getting Greener 20 Years Ago - D...   \n",
       "4   Why Solar Activity And Cosmic Rays Can’t Expla...   \n",
       "..                                                ...   \n",
       "1   \"I have never been an environmentalist. I don’...   \n",
       "2   The phrase “Mormon environmentalist” is not on...   \n",
       "3   Iceland's new prime minister is a feminist and...   \n",
       "5   Climate change is the biggest story on any edi...   \n",
       "6   @EileenOttawa thanx Eileen. Don't meet many fe...   \n",
       "\n",
       "                       mentions  replies_count  retweets_count  likes_count  \\\n",
       "0               ['climateadam']              1               5            9   \n",
       "1                ['johnfocook']              0              11           22   \n",
       "2   ['naomioreskes', 'latimes']              1              10           17   \n",
       "3                     ['sciam']              2              25           31   \n",
       "4            ['drshepherd2013']              1              20           32   \n",
       "..                          ...            ...             ...          ...   \n",
       "1                            []              3               9            9   \n",
       "2                            []              0               1            1   \n",
       "3                            []              1              24           47   \n",
       "5                            []              3              13           13   \n",
       "6              ['eileenottawa']              0               0            0   \n",
       "\n",
       "   hashtags        username       search_term  label  \\\n",
       "0        []  skepticscience    global warming     -1   \n",
       "1        []  skepticscience    global warming     -1   \n",
       "2        []  skepticscience    global warming     -1   \n",
       "3        []  skepticscience    global warming     -1   \n",
       "4        []  skepticscience    global warming     -1   \n",
       "..      ...             ...               ...    ...   \n",
       "1        []  skepticscience  environmentalist     -1   \n",
       "2        []  skepticscience  environmentalist     -1   \n",
       "3        []  skepticscience  environmentalist     -1   \n",
       "5        []  skepticscience  environmentalist     -1   \n",
       "6        []  skepticscience  environmentalist     -1   \n",
       "\n",
       "                                          tweet_clean  \n",
       "0   new video from climateadam tipping points coul...  \n",
       "1   how organized climate change denial shapes pub...  \n",
       "2   opinion can we please base our climate change ...  \n",
       "3   earth stopped getting greener 20 years ago  de...  \n",
       "4   why solar activity and cosmic rays cant explai...  \n",
       "..                                                ...  \n",
       "1   i have never been an environmentalist i dont e...  \n",
       "2   the phrase mormon environmentalist is not one ...  \n",
       "3   icelands new prime minister is a feminist and ...  \n",
       "5   climate change is the biggest story on any edi...  \n",
       "6   eileenottawa thanx eileen dont meet many fello...  \n",
       "\n",
       "[4233 rows x 13 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_deny_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import manually labelled & influential tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1507, 990, 277)\n",
      "253\n"
     ]
    }
   ],
   "source": [
    "# TODO: replace w/ own manually labeled tweets\n",
    "test_tweets = tweet_imports(\"Datasets/Correct Manual Labels/Manual_Labelled_Tweets.csv\")\n",
    "print(len(test_tweets), len(test_tweets[test_tweets['label']==1]), len(test_tweets[test_tweets['label']==-1]))\n",
    "x = test_tweets[test_tweets['label']==-1]\n",
    "print(len(x.drop_duplicates('tweet_clean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58686 54453 4233\n"
     ]
    }
   ],
   "source": [
    "# train1 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_1.csv\")\n",
    "# train2 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_2.csv\")\n",
    "# train3 = tweet_imports(\"Datasets/Training Data/influential_tweets_filter_3.csv\")\n",
    "# all_train = pd.concat([train1, train2, train3])\n",
    "all_train = pd.concat([cleaned_affirm_tweets, cleaned_deny_tweets])\n",
    "\n",
    "# Allison et al. had: (403432, 220063, 183369)\n",
    "print(len(all_train), len(all_train[all_train['label']==1]), len(all_train[all_train['label']==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29115\n"
     ]
    }
   ],
   "source": [
    "# # Select tweets that contain disasters from the training data \n",
    "# disaster_train = all_train[all_train['tweet_clean'].str.contains(\"michael|florence|wildfire|blizzard|fire|hurricane|bomb|cyclone|storm|snow|blaze\")==True]\n",
    "# disaster_train = disaster_train[disaster_train['tweet_clean'].str.contains(\"climate|change|global|warming\")==True]\n",
    "sub_train = all_train[all_train['tweet_clean'].str.contains(\"climate|change|global|warming\")==True]\n",
    "\n",
    "# Remove from training data the tweets that are already in the test data\n",
    "#dis_set = disaster_train[['tweet_clean', 'label']].copy()\n",
    "sub_set = sub_train[['tweet_clean', 'label']].copy()\n",
    "#test_tweets = test_tweets[['tweet_clean', 'label']].copy()\n",
    "#dis_set['identifier'] = 0\n",
    "sub_set['identifier'] = 0\n",
    "#test_tweets['identifier'] = 1\n",
    "#dis_set = pd.concat([dis_set, test_tweets])\n",
    "#sub_set = pd.concat([sub_set,test_tweets])\n",
    "#dis_set.drop_duplicates(keep=False)\n",
    "sub_set.drop_duplicates(keep='first')\n",
    "#disaster_train = dis_set[dis_set['identifier']==0]\n",
    "sub_train = sub_set[sub_set['identifier']==0]\n",
    "#print(len(disaster_train))\n",
    "print(len(sub_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & clean downloaded Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_restrict(filename, start_date, end_date):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets = tweet_imports(\"Datasets/Twint Output/\" + filename)\n",
    "     \n",
    "    # Constrain to relevant dates\n",
    "    print(min(tweets['date']), max(tweets['date']))\n",
    "    \n",
    "    begin_tweets = pd.to_datetime(start_date) - timedelta(weeks = 2)\n",
    "    end_tweets = pd.to_datetime(end_date) + timedelta(weeks = 2)\n",
    "    print(\"Two weeks before:\", begin_tweets, \"Two weeks after:\", end_tweets)\n",
    "    \n",
    "    tweets = tweets[tweets['date'] >= begin_tweets]\n",
    "    tweets = tweets[tweets['date'] <= end_tweets]\n",
    "\n",
    "    # Remove tweets to label that were already seen in train/valid/test for forming predictions\n",
    "    tweet_dis_overlap = tweets.merge(disaster_train, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_dis_overlap.tweet_clean))]\n",
    "    tweet_test_overlap = tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "    tweets = tweets[(~tweets.tweet_clean.isin(tweet_test_overlap.tweet_clean))]\n",
    "\n",
    "    # Combine pre-labeled tweets\n",
    "    pre_labelled_tweets = pd.concat((tweet_dis_overlap, tweet_test_overlap), axis=0)\n",
    "    print(len(tweets), len(pre_labelled_tweets))\n",
    "    \n",
    "    return((tweets, pre_labelled_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict(filename, start_date, end_date, outfile):\n",
    "    \n",
    "    tweets, pre_labelled_tweets = clean_tweets_restrict(filename, start_date, end_date)\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets to label\", len(tweets), \"Prior tweets to label\", len(pre), \"Post tweets to label\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets_restrict_combine(filename1, filename2, start_date, end_date, outfile):\n",
    "    \n",
    "    # Unlabeled tweets\n",
    "    tweets1, pre_labelled_tweets1 = clean_tweets_restrict(filename1, start_date, end_date)\n",
    "    tweets2, pre_labelled_tweets2 = clean_tweets_restrict(filename2, start_date, end_date)\n",
    "    print(len(tweets1))\n",
    "    print(len(tweets2))\n",
    "    tweets = pd.merge(tweets1, tweets2, how='outer')\n",
    "    pre_labelled_tweets = pd.merge(pre_labelled_tweets1, pre_labelled_tweets2, how='outer')\n",
    "    \n",
    "    # Split to pre- and post\n",
    "    pre = tweets[tweets['date'] <= start_date]\n",
    "    post = tweets[tweets['date'] > start_date]\n",
    "    pre.reset_index(inplace=True)\n",
    "    post.reset_index(inplace=True)\n",
    "    print(\"Total tweets\", len(tweets), \"Prior tweets\", len(pre), \"Post tweets\", len(post))\n",
    "    \n",
    "    # Merge pre- and post- tweets by same user to see if user sentiments change\n",
    "    pre_users = pd.DataFrame(pre['user_id'].unique())\n",
    "    post_users = pd.DataFrame(post['user_id'].unique())\n",
    "    merge = pd.merge(pre_users, post_users, how='inner')\n",
    "    print(\"Number of users tweeting before and after\", len(merge))\n",
    "\n",
    "    pre_tweets = pre.loc[pre['user_id'].isin(merge.iloc[:,0])]\n",
    "    post_tweets = post.loc[post['user_id'].isin(merge.iloc[:,0])]\n",
    "    print(\"Num tweets before\", len(pre_tweets), \"Num tweets after\", len(post_tweets))\n",
    "    \n",
    "    tweets.to_csv('Datasets/Event Tweets/' + outfile + '.csv', sep=',')\n",
    "    pre_labelled_tweets.to_csv('Datasets/Event Tweets/Prelabelled_' + outfile + '.csv', sep=',')\n",
    "    \n",
    "    return((tweets, pre, post, merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2017-11-21 00:00:00'), Timestamp('2018-01-19 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2017-12-19 00:00:00'), 'Two weeks after:', Timestamp('2018-01-20 00:00:00'))\n",
      "(14957, 1610)\n",
      "('Total tweets to label', 14957, 'Prior tweets to label', 2614, 'Post tweets to label', 12343)\n",
      "('Number of users tweeting before and after', 330)\n",
      "('Num tweets before', 634, 'Num tweets after', 1479)\n"
     ]
    }
   ],
   "source": [
    "# January 2018 bomb cyclone (Jan 2 - Jan 6): https://en.wikipedia.org/wiki/January_2018_North_American_blizzard\n",
    "blizzard_tweets, blizzard_pre, blizzard_post, blizzard_merge = count_tweets_restrict('blizzard_geo_tweets_v2.csv', '1/2/18', '1/6/18', 'blizzard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-05-31 00:00:00'), Timestamp('2018-09-26 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2710, 929)\n",
      "(Timestamp('2018-06-04 00:00:00'), Timestamp('2018-10-01 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-07-13 00:00:00'), 'Two weeks after:', Timestamp('2018-10-02 00:00:00'))\n",
      "(2808, 956)\n",
      "2710\n",
      "2808\n",
      "('Total tweets', 3035, 'Prior tweets', 173, 'Post tweets', 2862)\n",
      "('Number of users tweeting before and after', 36)\n",
      "('Num tweets before', 49, 'Num tweets after', 95)\n"
     ]
    }
   ],
   "source": [
    "# California Mendocino Wildfires (July 27 - Sep 18): https://en.wikipedia.org/wiki/Mendocino_Complex_Fire\n",
    "summerfire_tweets, summerfire_pre, summerfire_post, summerfire_merge = count_tweets_restrict_combine('summerfire_geo_tweets.csv', \n",
    "                                                                                             'summerfire_geo_tweets_v2.csv', \n",
    "                                                                                             '7/27/18', '9/18/18',\n",
    "                                                                                                    'summerfire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-07-03 00:00:00'), Timestamp('2018-09-30 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-08-17 00:00:00'), 'Two weeks after:', Timestamp('2018-10-03 00:00:00'))\n",
      "(6413, 1032)\n",
      "('Total tweets to label', 6413, 'Prior tweets to label', 778, 'Post tweets to label', 5635)\n",
      "('Number of users tweeting before and after', 122)\n",
      "('Num tweets before', 193, 'Num tweets after', 497)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Florence (Aug 31 - Sep 19): https://en.wikipedia.org/wiki/Hurricane_Florence\n",
    "florence_tweets, florence_pre, florence_post, florence_merge = count_tweets_restrict('florence_geo_tweets.csv', '8/31/18', '9/19/18',\n",
    "                                                                                    'florence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-05 00:00:00'), Timestamp('2018-10-28 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-09-23 00:00:00'), 'Two weeks after:', Timestamp('2018-10-30 00:00:00'))\n",
      "(13035, 3126)\n",
      "('Total tweets to label', 13035, 'Prior tweets to label', 1912, 'Post tweets to label', 11123)\n",
      "('Number of users tweeting before and after', 281)\n",
      "('Num tweets before', 880, 'Num tweets after', 1351)\n"
     ]
    }
   ],
   "source": [
    "# Hurricane Michael (Oct 7 - Oct 16): https://en.wikipedia.org/wiki/Hurricane_Michael\n",
    "michael_tweets, michael_pre, michael_post, michael_merge = count_tweets_restrict('michael_geo_tweets.csv', '10/07/18', '10/16/18',\n",
    "                                                                                'michael')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(5375, 151)\n",
      "(Timestamp('2018-09-10 00:00:00'), Timestamp('2018-12-08 00:00:00'))\n",
      "('Two weeks before:', Timestamp('2018-10-25 00:00:00'), 'Two weeks after:', Timestamp('2018-12-09 00:00:00'))\n",
      "(6081, 143)\n",
      "5375\n",
      "6081\n",
      "('Total tweets', 6654, 'Prior tweets', 55, 'Post tweets', 6599)\n",
      "('Number of users tweeting before and after', 14)\n",
      "('Num tweets before', 19, 'Num tweets after', 43)\n"
     ]
    }
   ],
   "source": [
    "# California Camp wildfires (Nov 8 - 25): https://en.wikipedia.org/wiki/Camp_Fire_(2018)\n",
    "winterfire_tweets, winterfire_pre, winterfire_post, winterfire_merge = count_tweets_restrict_combine(\n",
    "    'winterfire_geo_tweets.csv', \n",
    "    'winterfire_geo_tweets_v2.csv', \n",
    "    '11/08/18', '11/25/18', 'winterfire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train / validation / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18191\n",
      "(8939, 9252)\n",
      "(990, 277)\n"
     ]
    }
   ],
   "source": [
    "#randomly select tweets that go to validation set or training set\n",
    "disaster_train = shuffle(disaster_train,random_state=123)\n",
    "disaster_train = disaster_train.drop_duplicates('tweet_clean')\n",
    "num_tweets = len(disaster_train)\n",
    "\n",
    "print(num_tweets)\n",
    "infl_val_pos = disaster_train[disaster_train['label']==1]\n",
    "infl_val_neg = disaster_train[disaster_train['label']==-1]\n",
    "print(len(infl_val_pos), len(infl_val_neg))\n",
    "\n",
    "labeled_tweets = test_tweets[test_tweets['label']!=0]\n",
    "test_tweets_shuffle = shuffle(labeled_tweets,random_state=456)\n",
    "manual_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "manual_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(manual_pos), len(manual_neg))\n",
    "\n",
    "train_pct = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "859\n",
      "252\n",
      "('Num total tweets', 18191, '\\n Num train tweets', 16371, '\\n Num validation tweets', 1820, '\\n Num test tweets', 500)\n"
     ]
    }
   ],
   "source": [
    "training_data = disaster_train[:int(num_tweets*train_pct)]\n",
    "\n",
    "val_tweets = disaster_train[int(num_tweets*train_pct):]\n",
    "\n",
    "# concatenate test tweets\n",
    "half_test_num = 250\n",
    "\n",
    "test_tweets_shuffle = test_tweets_shuffle.drop_duplicates('tweet_clean')\n",
    "print(len(test_tweets_shuffle))\n",
    "test_tweets_pos = test_tweets_shuffle[test_tweets_shuffle['label']==1]\n",
    "print(len(test_tweets_pos))\n",
    "test_tweets_neg = test_tweets_shuffle[test_tweets_shuffle['label']==-1]\n",
    "print(len(test_tweets_neg))\n",
    "\n",
    "test_tweets = shuffle(pd.concat([test_tweets_neg[:half_test_num],test_tweets_pos[:half_test_num]]),random_state=0)\n",
    "\n",
    "print(\"Num total tweets\", num_tweets,\n",
    "      \"\\n Num train tweets\", len(training_data), \n",
    "      \"\\n Num validation tweets\", len(val_tweets), \n",
    "      \"\\n Num test tweets\", len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num train tweets', 16371, 'Num positive tweets', 8048, 'Num negative tweets', 8323)\n",
      "('Num val tweets', 1820, 'Num positive tweets', 891, 'Num negative tweets', 929)\n",
      "('Num test tweets', 500, 'Num positive tweets', 250, 'Num negative tweets', 250)\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))\n",
    "\n",
    "test_tweets = pd.concat([test_tweets_pos, test_tweets_neg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test tweets from training & validation sets\n",
    "test_train_overlap = training_data.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_train_overlap))\n",
    "training_data = training_data[(~training_data.tweet_clean.isin(test_train_overlap.tweet_clean))]\n",
    "#print(len(training_data))\n",
    "\n",
    "test_val_overlap = val_tweets.merge(test_tweets, on=['tweet_clean'])\n",
    "#print(len(test_val_overlap))\n",
    "val_tweets = val_tweets[(~val_tweets.tweet_clean.isin(test_val_overlap.tweet_clean))]\n",
    "#print(len(val_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num train tweets', 16360, 'Num positive tweets', 8044, 'Num negative tweets', 8316)\n",
      "('Num val tweets', 1819, 'Num positive tweets', 891, 'Num negative tweets', 928)\n",
      "('Num test tweets', 500, 'Num positive tweets', 250, 'Num negative tweets', 250)\n"
     ]
    }
   ],
   "source": [
    "# Basic stats on training data from celebrities\n",
    "train_pos = training_data[training_data['label']==1]\n",
    "train_neg = training_data[training_data['label']==-1]\n",
    "print(\"Num train tweets\", len(training_data), \"Num positive tweets\", len(train_pos), \n",
    "      \"Num negative tweets\", len(train_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "val_tweets_pos = val_tweets[val_tweets['label']==1]\n",
    "val_tweets_neg = val_tweets[val_tweets['label']==-1]\n",
    "print(\"Num val tweets\", len(val_tweets), \"Num positive tweets\", len(val_tweets_pos), \n",
    "      \"Num negative tweets\", len(val_tweets_neg))\n",
    "\n",
    "# Check number of manual labelled tweets and tweets to label\n",
    "test_tweets_pos = test_tweets[test_tweets['label']==1]\n",
    "test_tweets_neg = test_tweets[test_tweets['label']==-1]\n",
    "print(\"Num test tweets\", len(test_tweets), \"Num positive tweets\", len(test_tweets_pos), \n",
    "      \"Num negative tweets\", len(test_tweets_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv('Datasets/SentimentTests/dedup_training_data.csv', sep=',')\n",
    "val_tweets.to_csv('Datasets/SentimentTests/dedup_val_data.csv', sep=',')\n",
    "test_tweets.to_csv('Datasets/SentimentTests/dedup_test_data.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
