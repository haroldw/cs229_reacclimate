{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "import os.path\n",
    "import os\n",
    "import sklearn\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import argparse\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Twitter credentials for the app\n",
    "consumer_key = 'Lydypy5GRHslhuWsXTAagVFpO'\n",
    "consumer_secret = 'K9HA6MyfRWm73G50WHvzBPxfY0gWfJRk5ajcUmGRCg4e9NiM69'\n",
    "access_key= '789687511-BGbhUzj8zVLk9HeKKxrCZnzJ21xb3qXqZMHyf0gX'\n",
    "access_secret = 'kIDmi6vhOiePyEIZ5XXrOV8rl0xLOe5wLQ2XbhH2qCLsr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#pass twitter credentials to tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#columns of the csv file\n",
    "COLS = ['id', 'created_at', 'screen_name', 'clean_text', 'original_text', 'favorite_count', 'retweet_count', 'follower_count', 'polarity','subjectivity', 'hashtags',\n",
    " 'location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    " \n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    tweet = re.sub(r'RT *[^:]*:','', tweet);\n",
    "    #tweet = re.sub(r'[\\.@[a-zA-Z]*:]+','', tweet);\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "#remove url\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "#remove more symbols\n",
    "    tweet = tweet.replace(\"-\", \"\")\n",
    "    tweet = tweet.replace(\"_\", \"\")\n",
    "    tweet = tweet.replace(\"*\", \"\")\n",
    "    tweet = tweet.replace(\".\", \"\")\n",
    "#filter using NLTK library append it to a string\n",
    "    filtered_tweet = []\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "#looping through conditions\n",
    "    for w in word_tokens:\n",
    "#check tokens against stop words , emoticons and punctuations\n",
    "        if w not in emoticons and w not in string.punctuation:\n",
    "            if w.isdigit():\n",
    "                continue\n",
    "            if w[0] == \".\":\n",
    "                continue\n",
    "            if '9' in w:\n",
    "                w = w.replace(\"9\", \"\")\n",
    "                w = re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', w)\n",
    "            \n",
    "            w = w.lower()\n",
    "            \n",
    "            if len(w) == 1 and w not in ['a', 'e', 'i','o','u']:\n",
    "                continue\n",
    "            elif len(w) == 2 and w[0] in string.punctuation:\n",
    "                continue\n",
    "            filtered_tweet.append(w.lower())\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = { \n",
    "\"ain't\": \"have not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he has\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how are\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she has\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there's\": \"there has\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"y'all\": \"you all\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()), flags = re.IGNORECASE)\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0).lower()]\n",
    "    return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def write_tweets(query, file):\n",
    "    #If the file exists, then read the existing data from the CSV file.\n",
    "    if os.path.isfile(file):\n",
    "        df = pd.read_csv(file)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=COLS)\n",
    "    #page attribute in tweepy.cursor and iteration\n",
    "    for page in tweepy.Cursor(api.search, q = query, count=200, lang = 'en', include_rts=False, tweet_mode = 'extended').pages(200):\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    "            \n",
    "            '''\n",
    "            if status['id'] in df['id'].values and status['created_at'] in df['created_at'].values:\n",
    "                i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
    "                if status['favorite_count'] != df.at[i, 'favorite_count'] or \\\n",
    "                status['retweet_count'] != df.at[i, 'retweet_count']:\n",
    "                    df.at[i, 'favorite_count'] = status['favorite_count']\n",
    "                    df.at[i, 'retweet_count'] = status['retweet_count']\n",
    "                    continue\n",
    "            '''\n",
    "            #filtering tweets by number of favorites\n",
    "            if status['favorite_count'] < 5:\n",
    "                continue\n",
    "            \n",
    "            #if \"retweeted_status\" in status:\n",
    "                #status['full_text'] = status[\"retweeted_status\"]['full_text']\n",
    "            \n",
    "            original_tweet = status['full_text']\n",
    "            \n",
    "            tweet = status['full_text'].replace(\"&amp;\", \"and\")\n",
    "            tweet = tweet.replace(\"’\", \"'\")\n",
    "            tweet = tweet.replace(\"#\", \"9\")\n",
    "            tweet = expand_contractions(tweet)\n",
    "       \n",
    "            p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.SMILEY)\n",
    "            clean_text = p.clean(tweet)\n",
    "            filtered_tweet=clean_tweets(clean_text)\n",
    "            \n",
    "            filtered_tweet = filtered_tweet.strip()\n",
    "            if len(filtered_tweet.split()) < 2:\n",
    "                continue\n",
    "\n",
    "            blob = TextBlob(filtered_tweet)\n",
    "            Sentiment = blob.sentiment     \n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    "            \n",
    "            new_entry += [status['id'], status['created_at'], status['user']['screen_name'], filtered_tweet, original_tweet, \\\n",
    "                          status['favorite_count'], status['retweet_count'], status['user']['followers_count'], polarity,subjectivity]\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags) #append the hashtags\n",
    "           \n",
    "            #append_locations\n",
    "            try:\n",
    "                location = status['user']['location']\n",
    "            except TypeError:\n",
    "                location = ''\n",
    "            new_entry.append(location)\n",
    "            \n",
    "            \n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            csvFile = open(file, 'a+' ,encoding='utf-8')\n",
    "       \n",
    "    df.to_csv(csvFile, mode='a', columns=COLS, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#declare file paths as follows for three files\n",
    "query1 = \"climate change -filter:retweets\"\n",
    "query2 = \"global warming -filter:retweets\"\n",
    "query3 = \"climate -change -filter:retweets\"\n",
    "\n",
    "\n",
    "CCtweets = \"/Users/User/229project/data/climate_change.csv\"\n",
    "GWtweets = \"/Users/User/229project/data/global_warming.csv\"\n",
    "Ctweets = \"/Users/User/229project/data/climate.csv\"\n",
    "\n",
    "\n",
    "write_tweets(query1, CCtweets)\n",
    "write_tweets(query2, GWtweets)\n",
    "write_tweets(query3, Ctweets)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Thoughts: \n",
    "    -We can limit the climate tweets we download  by num of favorites, num retweets, or num followers, \n",
    "        or a combination of those...this actually cuts the data by a lot...the vast majority of tweets\n",
    "        gathered have 0 likes and retweets...there are a LOT of tweets about climate so it might make sense to pick \n",
    "        the more popular tweets. What should be that threshold? As an example:\n",
    "        with no limitation on the number of favorites, 100 pages of download was 10k tweets for query 'climate change' \n",
    "        all from the same day I downloaded...but with even a limit of at least 2 favorites, the number of tweets \n",
    "        for 100 pages was cut to about 2k. Currently I set the threshold to 5 and am looking at 200 pages \n",
    "        of tweets starting from today, May 10th.\n",
    "    -Is there any particular date we want to get tweets from...again, there are so many postings that most of \n",
    "        the tweets we'll get will be on that date. \n",
    "    -Engagement can be measured by some sort of ratio of number of sentiment/likes/rts to number of followers\n",
    "        -Do we even need the sentiment of the replies? Getting replies isn't nicely supported and I think ratio of \n",
    "        likes/rts to followers says more\n",
    "        -If we just blanket download a lot of \"base\" climate change tweets, and then download all of the \n",
    "        climate-focused tweets from all the users in the \"base\" tweets, shouldn't that be enough? Also, these tweets\n",
    "        will include replies, it just wont be directly tied to the tweet its replying to.h\n",
    "    -How many of these \"base\" tweets/users do we get...from which we'll get their climate-focused tweet histories...\n",
    "        -Should we get \"base\" tweets at different time points?\n",
    "        \n",
    "Next steps:\n",
    "    -Answer above questions\n",
    "    -Re-download a lot more base tweets\n",
    "    -Extract the thousands of screen_names from base tweets and download their climate tweet history\n",
    "        --this will involve running individual queries on an array of thousands of handles...I'm currently\n",
    "        already running into 429 \"too many request errors\" which slowwwwss down the download, so may need to\n",
    "        figure out how to work around this. I think it may not exist with a premium account.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
